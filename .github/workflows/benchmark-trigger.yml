  name: Benchmark Trigger

  on:
    repository_dispatch:
      types: [new_ai_benchmark_build]

  jobs:
    run-benchmark:
      runs-on: ubuntu-latest
      permissions:
        contents: write
        packages: read

      steps:
        - name: Checkout code
          uses: actions/checkout@v4
          with:
            token: ${{ secrets.GITHUB_TOKEN }}

        - name: Set up Docker Buildx
          uses: docker/setup-buildx-action@v3

        - name: Log in to GitHub Container Registry
          uses: docker/login-action@v3
          with:
            registry: ghcr.io
            username: ${{ github.actor }}
            password: ${{ secrets.GITHUB_TOKEN }}

        - name: Extract Metadata from Image
          run: |
            # Set IMAGE_NAME from payload
            IMAGE_NAME="${{ github.event.client_payload.image_name }}"
            echo "IMAGE_NAME=$IMAGE_NAME" >> $GITHUB_ENV
            
            # Debug: Print the IMAGE_NAME
            echo "Debug: IMAGE_NAME='$IMAGE_NAME'"
            
            # Validation: Fail the build if IMAGE_NAME is empty
            if [ -z "$IMAGE_NAME" ]; then
              echo "ERROR: IMAGE_NAME is empty in payload. Check the repository dispatch payload."
              echo "Available payload keys:"
              echo "${{ toJson(github.event.client_payload) }}"
              exit 1
            fi
            
            docker pull "$IMAGE_NAME"
            
            # Extract labels
            RAW_COMMIT=$(docker inspect --format='{{index .Config.Labels "org.opencontainers.image.revision"}}' "$IMAGE_NAME")
            RAW_TAG=$(docker inspect --format='{{index .Config.Labels "org.opencontainers.image.ref.name"}}' "$IMAGE_NAME")
            
            # Validation: Fail the build if COMMIT is empty
            if [ -z "$RAW_COMMIT" ] || [ "$RAW_COMMIT" == "<no value>" ]; then
            echo "ERROR: OCI Revision (commit) missing in image metadata. Ensure Gradle build passes -PGIT_COMMIT."
            exit 1
            fi
            
            # Validation: Fail the build if TAG is empty
            if [ -z "$RAW_TAG" ] || [ "$RAW_TAG" == "<no value>" ]; then
            echo "ERROR: OCI Ref Name (tag) missing in image metadata. Ensure Gradle build passes -PGIT_TAG."
            exit 1
            fi
            
            echo "APP_GIT_COMMIT=$RAW_COMMIT" >> $GITHUB_ENV
            echo "APP_GIT_TAG=$RAW_TAG" >> $GITHUB_ENV

        - name: Write Spring environment file
          run: |
            # Combine main spring environment and google account samples
            echo '${{ secrets.SPRING_ENV_FILE_CONTENT }}' > .env.spring.benchmark.part1
            echo '${{ secrets.GOOGLE_ACCOUNT_SAMPLES }}' > .env.spring.benchmark.part2
            
            # Combine both parts into final file
            cat .env.spring.benchmark.part1 .env.spring.benchmark.part2 > .env.spring.benchmark
            
            # Clean up temporary files
            rm .env.spring.benchmark.part1 .env.spring.benchmark.part2

        - name: Set up JDK
          uses: actions/setup-java@v4
          with:
            java-version: '21'
            distribution: 'temurin'

        - name: Run Benchmark
          env:
            # Only benchmark-specific variables
            APP_GIT_COMMIT: ${{ env.APP_GIT_COMMIT }}
            APP_GIT_TAG: ${{ env.APP_GIT_TAG }}
            IMAGE_NAME: ${{ env.IMAGE_NAME }}

            # AI Cost Benchmark configuration
            GEMINI_3_FLASH_PREVIEW_INPUT_TOKEN_PRICE_PER_MILLION: ${{ vars.GEMINI_INPUT_TOKEN_PRICE }}
            GEMINI_3_FLASH_PREVIEW_OUTPUT_TOKEN_PRICE_PER_MILLION: ${{ vars.GEMINI_OUTPUT_TOKEN_PRICE }}
            AI_BENCHMARK_ENABLE_VERBOSE_DOCKER_LOGS: ${{ vars.AI_BENCHMARK_ENABLE_VERBOSE_DOCKER_LOGS }}
            AI_BENCHMARK_ENDPOINT: ${{ vars.AI_BENCHMARK_ENDPOINT }}
            AI_BENCHMARK_K6_ITERATIONS: ${{ vars.AI_BENCHMARK_K6_ITERATIONS }}
            AI_BENCHMARK_K6_WARMUP_ITERATIONS: ${{ vars.AI_BENCHMARK_K6_WARMUP_ITERATIONS }}
            AI_BENCHMARK_REQUEST_TIMEOUT: ${{ vars.AI_BENCHMARK_REQUEST_TIMEOUT }}
          run: |
            chmod +x run-ai-benchmark.sh
            ./run-ai-benchmark.sh

        - name: Upload Results
          uses: actions/upload-artifact@v4
          if: always()
          with:
            name: ai-benchmark-results-${{ env.APP_GIT_COMMIT }}
            path: |
              results/ai-benchmark/
              results/reports/
            retention-days: 30

        - name: Add Column to Existing README Report
          if: success()
          env:
            # AI Cost Benchmark configuration
            GEMINI_3_FLASH_PREVIEW_INPUT_TOKEN_PRICE_PER_MILLION: ${{ vars.GEMINI_3_FLASH_PREVIEW_INPUT_TOKEN_PRICE_PER_MILLION }}
            GEMINI_3_FLASH_PREVIEW_OUTPUT_TOKEN_PRICE_PER_MILLION: ${{ vars.GEMINI_3_FLASH_PREVIEW_OUTPUT_TOKEN_PRICE_PER_MILLION }}
          run: |
            echo "üìä Adding new benchmark column to existing README.md report..."
            
            # Check if README.md has benchmark table structure
            if ! grep -q "<!-- BENCHMARK_RESULTS_START -->" README.md; then
              echo "‚ö†Ô∏è README.md doesn't have benchmark table structure. Creating initial report first..."
              # Use deprecated script to create initial report
              BENCHMARK_DIR="results/ai-benchmark/${{ env.APP_GIT_COMMIT }}/$(ls results/ai-benchmark/${{ env.APP_GIT_COMMIT }})"
              python scripts/create_initial_benchmark_report.py --commit ${{ env.APP_GIT_COMMIT }} --dir "$BENCHMARK_DIR"
              
              # Inject initial report into README
              LATEST_REPORT=$(ls -t results/reports/*.md | head -n 1)
              awk '
                BEGIN {inside=0}
                /<!-- BENCHMARK_RESULTS_START -->/ {
                  print
                  while ((getline line < "'"$LATEST_REPORT"'") > 0)
                    print line
                  close("'"$LATEST_REPORT"'")
                  inside=1
                  next
                }
                /<!-- BENCHMARK_RESULTS_END -->/ {
                  inside=0
                }
                !inside
              ' README.md > README.tmp
              mv README.tmp README.md
            else
              echo "üìù Adding column to existing benchmark table..."
              # Find the benchmark data directory for current commit
              BENCHMARK_DIR="results/ai-benchmark/${{ env.APP_GIT_COMMIT }}/$(ls results/ai-benchmark/${{ env.APP_GIT_COMMIT }})"
              
              # Create intermediate report file in results/reports/
              TIMESTAMP=$(date +"%Y-%m-%d_%H-%M-%S")
              INTERMEDIATE_REPORT="results/reports/ai-benchmark_${{ env.APP_GIT_COMMIT }}_intermediate_${TIMESTAMP}.md"
              
              # First, extract existing README.md benchmark section as initial content
              echo "üìÑ Extracting existing benchmark section from README.md..."
              awk '
                /<!-- BENCHMARK_RESULTS_START -->/, /<!-- BENCHMARK_RESULTS_END -->/ {
                  if ($0 !~ /<!-- BENCHMARK_RESULTS_.*-->/) {
                    print
                  }
                }
              ' README.md > "$INTERMEDIATE_REPORT"
              
              echo "‚úÖ Initial content extracted to $INTERMEDIATE_REPORT"
              
              # Add new column to intermediate report
              python scripts/trace_ai_benchmark_comparison.py \
                --existing-report "$INTERMEDIATE_REPORT" \
                --commit ${{ env.APP_GIT_COMMIT }} \
                --dir "$BENCHMARK_DIR"
              
              echo "‚úÖ Intermediate report updated with new column"
              
              # Update README.md with the same content
              python scripts/trace_ai_benchmark_comparison.py \
                --existing-report "README.md" \
                --commit ${{ env.APP_GIT_COMMIT }} \
                --dir "$BENCHMARK_DIR"
              
              echo "‚úÖ README.md updated successfully"
            fi
            echo "‚úÖ Benchmark report updated"

        - name: Display Benchmark Results
          run: |
            # Extract and display benchmark section from README.md
            if [ -f "README.md" ]; then
              awk '
                /<!-- BENCHMARK_RESULTS_START -->/, /<!-- BENCHMARK_RESULTS_END -->/ {
                  if ($0 !~ /<!-- BENCHMARK_RESULTS_.*-->/) {
                    print
                  }
                }
              ' README.md >> $GITHUB_STEP_SUMMARY
            else
              # Fallback to report files if README.md doesn't exist
              cat results/reports/*.md >> $GITHUB_STEP_SUMMARY 2>/dev/null || echo "No benchmark reports found" >> $GITHUB_STEP_SUMMARY
            fi

        - name: Commit and Push README
          run: |
            git config --global user.name "github-actions[bot]"
            git config --global user.email "github-actions[bot]@users.noreply.github.com"
            git add README.md
            git commit -m "docs: auto-update benchmark results [skip ci]" || echo "No changes"
            git push